{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Breakout_kears_Testing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9y8XeOmoRUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ.setdefault('PATH', '')\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import spaces\n",
        "import cv2\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so it's important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip       = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
        "        \"\"\"\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
        "        observation should be warped.\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grayscale = grayscale\n",
        "        self._key = dict_space_key\n",
        "        if self._grayscale:\n",
        "            num_colors = 1\n",
        "        else:\n",
        "            num_colors = 3\n",
        "\n",
        "        new_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(self._height, self._width, num_colors),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        if self._key is None:\n",
        "            original_space = self.observation_space\n",
        "            self.observation_space = new_space\n",
        "        else:\n",
        "            original_space = self.observation_space.spaces[self._key]\n",
        "            self.observation_space.spaces[self._key] = new_space\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
        "\n",
        "    def observation(self, obs):\n",
        "        if self._key is None:\n",
        "            frame = obs\n",
        "        else:\n",
        "            frame = obs[self._key]\n",
        "\n",
        "        if self._grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        if self._grayscale:\n",
        "            frame = np.expand_dims(frame, -1)\n",
        "\n",
        "        if self._key is None:\n",
        "            obs = frame\n",
        "        else:\n",
        "            obs = obs.copy()\n",
        "            obs[self._key] = frame\n",
        "        return obs\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.env.render(mode='rgb_array')\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "    def count(self):\n",
        "        frames = self._force()\n",
        "        return frames.shape[frames.ndim - 1]\n",
        "\n",
        "    def frame(self, i):\n",
        "        return self._force()[..., i]\n",
        "\n",
        "\n",
        "class TimeLimit(gym.Wrapper):\n",
        "    def __init__(self, env, max_episode_steps=None):\n",
        "        super(TimeLimit, self).__init__(env)\n",
        "        self._max_episode_steps = max_episode_steps\n",
        "        self._elapsed_steps = 0\n",
        "\n",
        "    def step(self, ac):\n",
        "        observation, reward, done, info = self.env.step(ac)\n",
        "        self._elapsed_steps += 1\n",
        "        if self._elapsed_steps >= self._max_episode_steps:\n",
        "            done = True\n",
        "            info['TimeLimit.truncated'] = True\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self._elapsed_steps = 0\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "def make_atari(env_id, max_episode_steps=None):\n",
        "    env = gym.make(env_id)\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    if max_episode_steps is not None:\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
        "    return env\n",
        "\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = gym.wrappers.Monitor(env, '/content/video_dir/', force=True)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return env\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgWqPpFi9zmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc12c144-a9fe-4b5b-a1ff-6ee1f5910da0"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (\n",
        "    epsilon_max - epsilon_min\n",
        ")  # Rate at which to reduce chance of random action being taken\n",
        "batch_size = 32  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
        "env.seed(seed)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42, 742738649]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpKIAuA3okwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_actions = 4\n",
        "\n",
        "\n",
        "def create_q_model():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = create_q_model()\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = create_q_model()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txgyZSCexat3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5c65aa9b-636e-4640-eabc-9fd218615d41"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_root = '/content/drive/My Drive/'\n",
        "checkpoint_dir = os.path.join(drive_root,'atari_checkpoints/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pni38CAnxZDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "46e758d3-0e74-40b2-84f8-22dbefadd7bf"
      },
      "source": [
        "checkpoint_path=os.path.join(checkpoint_dir,'model-'+str(6131044)+'.h5')\n",
        "if os.path.exists(checkpoint_path):\n",
        "  model=keras.models.load_model(checkpoint_path)\n",
        "  print('Successfully loaded weights')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Successfully loaded weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGCf9a9GwcTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(agent, env, total_episodes=30):\n",
        "    rewards = []\n",
        "    env.seed(seed)\n",
        "    for i in range(total_episodes):\n",
        "        state = np.array(env.reset())\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        #playing one game\n",
        "        while(not done):\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "            state, reward, done, info = env.step(action)\n",
        "            state = np.array(state)\n",
        "            episode_reward += reward\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "    print('Run %d episodes'%(total_episodes))\n",
        "    print('Mean:', np.mean(rewards))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XbEYUCewjrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e73bfc28-ffea-4d13-c9ef-f0152366ad3e"
      },
      "source": [
        "test(model,env,30)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run 30 episodes\n",
            "Mean: 11.266666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTKVVrxY3O5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1e9310ed-c5b5-4cd3-8a9c-6bec5b4554fd"
      },
      "source": [
        "!zip -r /content/video.zip /content/video_dir"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: content/video_dir/ (stored 0%)\n",
            "updating: content/video_dir/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000001.mp4 (deflated 22%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000008.meta.json (deflated 60%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000008.mp4 (deflated 23%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000000.mp4 (deflated 27%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000001.meta.json (deflated 60%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000000.meta.json (deflated 60%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000027.meta.json (deflated 60%)\n",
            "  adding: content/video_dir/openaigym.video.0.2106.video000027.mp4 (deflated 23%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Nqc6cY3Z4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "97eefe19-76fb-40fe-82c7-61e3fa146423"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/video.zip\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6f6a634a-a478-4a5c-90bd-35160457d2d9\", \"video.zip\", 455255)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}